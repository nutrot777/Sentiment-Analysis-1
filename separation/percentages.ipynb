{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/nutrot/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import string \n",
    "import nltk \n",
    "nltk.download('vader_lexicon')\n",
    "import warnings \n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning )\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning \n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    #filter to allow only aphabets\n",
    "    text = re.sub(r'[^a-zA-z\\']', ' ', text)\n",
    "    #remove unicode characters \n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    #remove special characters, numbers, punctuations\n",
    "   # text = re.sub(r'[^a-zA-Z#\\']', ' ', text)\n",
    "    #removing shortwords\n",
    "    text = ' '.join([w for w in text.split() if len(w)>3])\n",
    "    #lower case \n",
    "    text = text.lower()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting words\n",
    "def extract(x):\n",
    "    words = []\n",
    "    #Loop over the words in the content \n",
    "    for i in x: \n",
    "        word = re.findall(r\"\\b\\w{4,225}\\b\",i)\n",
    "        words.append(word)\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent(pos, neg):\n",
    "    total = len(pos) + len(neg)\n",
    "    \n",
    "    #percentage of positive and negative \n",
    "    print(\"Total: \", total)\n",
    "    print(\"Positive: \", (len(pos)/total)*100)\n",
    "    print(\"Negative: \", (len(neg)/total)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Whole \n",
    "def twhole(path):\n",
    "    dataset = pd.read_csv(str(path))\n",
    "    \n",
    "    #dataset = dataset.drop(columns=['reviewId','userImage', 'thumbsUpCount', 'reviewCreatedVersion', 'at', 'replyContent', 'repliedAt'])\n",
    "    #dataset = dataset.drop(columns=['Company_Name', 'URL', 'Category_URL', 'Time'])\n",
    "    dataset = dataset.drop(columns=['isEdited', 'title', 'date', 'developerResponse'])\n",
    "    \n",
    "    #dataset['clean_review'] = dataset.content.apply(lambda x: np.vectorize(clean_text)(x))\n",
    "    #dataset['clean_review'] = dataset.Comments.apply(lambda x: np.vectorize(clean_text)(x))\n",
    "    dataset['clean_review'] = dataset.review.apply(lambda x: np.vectorize(clean_text)(x))\n",
    "    \n",
    "    dataset = dataset.dropna()\n",
    "    \n",
    "    words = []\n",
    "    #positive_w = extract(dataset['clean_review'][dataset['score']>3])\n",
    "    #negative_w = extract(dataset['clean_review'][dataset['score']<3])\n",
    "    #positive_w = extract(dataset['clean_review'][dataset['Star_rating']>3])\n",
    "    #negative_w = extract(dataset['clean_review'][dataset['Star_rating']<3])\n",
    "    \n",
    "    positive_w = extract(dataset['clean_review'][dataset['rating']>3])\n",
    "    negative_w = extract(dataset['clean_review'][dataset['rating']<3])\n",
    "    \n",
    "    positive_w = sum(positive_w, [])\n",
    "    negative_w = sum(negative_w, [])\n",
    "    for word in positive_w:\n",
    "        words.append(word)\n",
    "    for word in negative_w:\n",
    "        words.append(word)\n",
    "        \n",
    "    word_freq = nltk.FreqDist(words)\n",
    "    dict_filter = lambda word_freq, stopwords: dict((w, word_freq[w]) for w in word_freq if w not in STOPWORDS)\n",
    "    relevant_words = dict_filter(word_freq, STOPWORDS)\n",
    "    \n",
    "    word_count = pd.DataFrame({'Freq_Word':list(relevant_words.keys()), \n",
    "                          'Count':list(relevant_words.values())})\n",
    "    \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    positive_sent =[]\n",
    "    negative_sent =[]\n",
    "    \n",
    "    for s in word_count['Freq_Word']:\n",
    "        if(sid.polarity_scores(s)['compound'])>0.5:\n",
    "            positive_sent.append(s)\n",
    "        \n",
    "        \n",
    "    for s in word_count['Freq_Word']:\n",
    "        if(sid.polarity_scores(s)['compound'])< -0.5:\n",
    "            negative_sent.append(s)\n",
    "        \n",
    "    percent(positive_sent, negative_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:  39\n",
      "Positive:  84.61538461538461\n",
      "Negative:  15.384615384615385\n"
     ]
    }
   ],
   "source": [
    "twhole(\"../../mHealth_apple/A55.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating               202\n",
       "isEdited             202\n",
       "review               202\n",
       "userName             202\n",
       "title                202\n",
       "date                 202\n",
       "developerResponse    161\n",
       "dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"../../mHealth_apple/A55.csv\")\n",
    "dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = dataset.drop(columns=['reviewId','userImage', 'thumbsUpCount', 'reviewCreatedVersion', 'at', 'replyContent', 'repliedAt'])\n",
    "dataset = dataset.drop(columns=['isEdited', 'title', 'date', 'developerResponse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset['clean_review'] = dataset.content.apply(lambda x: np.vectorize(clean_text)(x))\n",
    "dataset['clean_review'] = dataset.review.apply(lambda x: np.vectorize(clean_text)(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "#positive_w = extract(dataset['clean_review'][dataset['score']>3])\n",
    "#negative_w = extract(dataset['clean_review'][dataset['score']<3])\n",
    "\n",
    "positive_w = extract(dataset['clean_review'][dataset['rating']>3])\n",
    "negative_w = extract(dataset['clean_review'][dataset['rating']<3])\n",
    "\n",
    "positive_w = sum(positive_w, [])\n",
    "negative_w = sum(negative_w, [])\n",
    "\n",
    "for w in positive_w:\n",
    "    words.append(w)\n",
    "for w in negative_w:\n",
    "    words.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26758"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = nltk.FreqDist(words)\n",
    "dict_filter = lambda word_freq, stopwords: dict((w, word_freq[w]) for w in word_freq if w not in STOPWORDS)\n",
    "relevant_words = dict_filter(word_freq, STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2926"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relevant_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = pd.DataFrame({'Freq_Word':list(relevant_words.keys()), \n",
    "                          'Count':list(relevant_words.values())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "positive_sent =[]\n",
    "negative_sent =[]\n",
    "#sent = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for s in word_count['Freq_Word']:\n",
    "    if(sid.polarity_scores(s)['compound'])>0.5:\n",
    "        positive_sent.append(s)\n",
    "\n",
    "len(positive_sent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['great',\n",
       " 'love',\n",
       " 'excellent',\n",
       " 'ideal',\n",
       " 'best',\n",
       " 'awesome',\n",
       " 'perfect',\n",
       " 'amazing',\n",
       " 'super',\n",
       " 'loved',\n",
       " 'free',\n",
       " 'brilliant',\n",
       " 'superb',\n",
       " 'wonderful',\n",
       " 'fantastic',\n",
       " 'kind',\n",
       " 'happy',\n",
       " 'perfectly',\n",
       " 'gain',\n",
       " 'encouraging',\n",
       " 'loving',\n",
       " 'lovely',\n",
       " 'beautiful',\n",
       " 'pleasing',\n",
       " 'honest',\n",
       " 'beauty',\n",
       " 'fabulous',\n",
       " 'lmao',\n",
       " 'trust',\n",
       " 'gorgeous',\n",
       " 'appreciated',\n",
       " 'cheer',\n",
       " 'appreciation',\n",
       " 'inspiration',\n",
       " 'loves',\n",
       " 'friendliest',\n",
       " 'beautifully',\n",
       " 'brilliantly',\n",
       " 'congrats',\n",
       " 'congratulations',\n",
       " 'impressive',\n",
       " 'comfortable',\n",
       " 'prize',\n",
       " 'lovers',\n",
       " 'kudos',\n",
       " 'hahaha',\n",
       " 'excels',\n",
       " 'superior',\n",
       " 'peace']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for s in word_count['Freq_Word']:\n",
    "    if(sid.polarity_scores(s)['compound'])< -0.5:\n",
    "        negative_sent.append(s)\n",
    "        \n",
    "len(negative_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent(pos, neg):\n",
    "    total = len(pos) + len(neg)\n",
    "    \n",
    "    #percentage of positive and negative \n",
    "    print(\"Positive: \", (len(pos)/total)*100)\n",
    "    print(\"Negative: \", (len(neg)/total)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive:  65.33333333333333\n",
      "Negative:  34.66666666666667\n"
     ]
    }
   ],
   "source": [
    "percent(positive_sent, negative_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
