{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/nutrot/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import string \n",
    "import nltk \n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import warnings \n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import csv\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning )\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning \n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    #filter to allow only aphabets\n",
    "    text = re.sub(r'[^a-zA-z\\']', ' ', text)\n",
    "    #remove unicode characters \n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    #remove special characters, numbers, punctuations\n",
    "   # text = re.sub(r'[^a-zA-Z#\\']', ' ', text)\n",
    "    #removing shortwords\n",
    "    text = ' '.join([w for w in text.split() if len(w)>3])\n",
    "    #lower case \n",
    "    text = text.lower()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting words\n",
    "def extract(x):\n",
    "    words = []\n",
    "    #Loop over the words in the content \n",
    "    for i in x: \n",
    "        word = re.findall(r\"\\b\\w{4,225}\\b\",i)\n",
    "        words.append(word)\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relevant words extraction \n",
    "def relevant(input_word):\n",
    "    word_freq = nltk.FreqDist(input_word)\n",
    "    dict_filter =  lambda word_freq, stopwords: dict((w, word_freq[w]) for w in word_freq if w not in STOPWORDS)\n",
    "    relevant_w = dict_filter(word_freq, STOPWORDS)\n",
    "    \n",
    "    return relevant_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_extraction(filename, appname):\n",
    "    \n",
    "    \n",
    "    \n",
    "    features = pd.read_csv('sentiments_comparison.csv')\n",
    "    dataset = pd.read_csv(str(filename))\n",
    "    dataset = dataset.drop(columns=['isEdited', 'title', 'date', 'developerResponse'])\n",
    "    dataset.dropna()\n",
    "    \n",
    "    dataset['clean_review'] = dataset.review.apply(lambda x: np.vectorize(clean_text)(x))\n",
    "    \n",
    "    positive_w = extract(dataset['clean_review'][dataset['rating']>3])\n",
    "    negative_w = extract(dataset['clean_review'][dataset['rating']<3])\n",
    "    positive_w = sum(positive_w, [])\n",
    "    negative_w = sum(negative_w, [])\n",
    "    \n",
    "    allWords = []\n",
    "    allWords.append(positive_w)\n",
    "    allWords.append(negative_w)\n",
    "    allWords = sum(allWords, [])\n",
    "    \n",
    "    relevant_w = relevant(allWords)\n",
    "    \n",
    "    relevant_words = pd.DataFrame({'Word_freq': list(relevant_w.keys()),\n",
    "                              'Word_Count':list(relevant_w.values()),\n",
    "                             })\n",
    "    \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    texts = []\n",
    "    #neg_word_list = []\n",
    "    #neu_word_list = [] \n",
    "    for word1 in relevant_words['Word_freq']:\n",
    "        if(sid.polarity_scores(word1)['compound'])>0.5 or (sid.polarity_scores(word1)['compound'])<-0.5:\n",
    "            texts.append(word1)\n",
    "            \n",
    "    \n",
    "    liking = []\n",
    "    trust = []\n",
    "    anger = []\n",
    "    sadness = []\n",
    "    fear = []\n",
    "    \n",
    "    \n",
    "    l = [] #liking synonyms\n",
    "    t = [] #trust synonyms\n",
    "    a = [] #anger synonyms\n",
    "    sa = [] #sad synonyms\n",
    "    f = [] #fear synonyms\n",
    "\n",
    "    for w in features.liking.values:\n",
    "        for syn in wordnet.synsets(str(w)):\n",
    "            for lemma in syn.lemmas():\n",
    "                l.append(lemma.name())\n",
    "    for txt in texts: \n",
    "        if txt in set(l):\n",
    "            liking.append(txt)\n",
    "    liking = list(dict.fromkeys(liking))\n",
    "    #################################################\n",
    "\n",
    "    for w1 in features.trust.values:\n",
    "        for syn_t in wordnet.synsets(str(w1)):\n",
    "            for lemma1 in syn_t.lemmas():\n",
    "                t.append(lemma1.name())\n",
    "    for txt in texts:\n",
    "        if txt in set(t):\n",
    "            trust.append(txt)\n",
    "    trust = list(dict.fromkeys(trust))\n",
    "    ################################################\n",
    "\n",
    "\n",
    "    for w4 in features.anger.values:\n",
    "        for syn_a in wordnet.synsets(str(w4)):\n",
    "            for lemma4 in syn_a.lemmas():\n",
    "                a.append(lemma4.name())\n",
    "\n",
    "    for txt in texts: \n",
    "        if txt in set(a):\n",
    "            anger.append(txt)\n",
    "    anger = list(dict.fromkeys(anger))\n",
    "    ######################################################\n",
    "\n",
    "    for w5 in features.sadness.values:\n",
    "        for syn_sa in wordnet.synsets(str(w5)):\n",
    "            for lemma5 in syn_sa.lemmas():\n",
    "                sa.append(lemma5.name())\n",
    "    for txt in texts:\n",
    "        if txt in set(sa):\n",
    "            sadness.append(txt)\n",
    "    sadness = list(dict.fromkeys(sadness))\n",
    "\n",
    "    ########################################################\n",
    "    for w6 in features.fear.values:\n",
    "        for syn_f in wordnet.synsets(str(w6)):\n",
    "            for lemma6 in syn_f.lemmas():\n",
    "                f.append(lemma6.name())\n",
    "\n",
    "    for txt in texts:\n",
    "        if txt in set(f):\n",
    "            fear.append(txt)\n",
    "    fear = list(dict.fromkeys(fear))\n",
    "    \n",
    "    for duplicate in liking:\n",
    "        if duplicate in trust:\n",
    "            trust.remove(duplicate)\n",
    "        \n",
    "    for duplicate in trust:\n",
    "        if duplicate in anger:\n",
    "            anger.remove(duplicate)\n",
    "\n",
    "    for duplicate in anger:\n",
    "        if duplicate in sadness:\n",
    "            sadness.remove(duplicate)\n",
    "\n",
    "\n",
    "    for duplicate in sadness:\n",
    "        if duplicate in fear:\n",
    "            fear.remove(duplicate)\n",
    "            \n",
    "    liking_sentiment = pd.DataFrame({'liking':liking})\n",
    "    trust_sentiment = pd.DataFrame({'trust':trust})\n",
    "    anger_sentiment = pd.DataFrame({'anger':anger})\n",
    "    sadness_sentiment = pd.DataFrame({'sadness':sadness})\n",
    "    fear_sentiment = pd.DataFrame({'fear':fear})\n",
    "\n",
    "\n",
    "    sentiments = pd.concat([liking_sentiment, trust_sentiment, anger_sentiment, sadness_sentiment, fear_sentiment], axis = 1)\n",
    "    \n",
    "     #save as csv file in specific folder\n",
    "    sentiments.to_csv(appname, index=None)\n",
    "    return sentiments\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['developerResponse'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-2a3a6b90befc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentiment_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../mHealth_apple_app2/A62.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'app_sentiments/A62_sentiment.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-09ecc6e4c5fc>\u001b[0m in \u001b[0;36msentiment_extraction\u001b[0;34m(filename, appname)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sentiments_comparison.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'isEdited'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'title'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'developerResponse'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3995\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3996\u001b[0m             \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3997\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3998\u001b[0m         )\n\u001b[1;32m   3999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3934\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3935\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3936\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3938\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3968\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3970\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3971\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   5015\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5017\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5018\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5019\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['developerResponse'] not found in axis\""
     ]
    }
   ],
   "source": [
    "sentiment_extraction('../../mHealth_apple_app2/A62.csv', 'app_sentiments/A62_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
