{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading vader_lexicon: <urlopen error [Errno -3]\n",
      "[nltk_data]     Temporary failure in name resolution>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import string \n",
    "import nltk \n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import warnings \n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import csv\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning )\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning \n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    #filter to allow only aphabets\n",
    "    text = re.sub(r'[^a-zA-z\\']', ' ', text)\n",
    "    #remove unicode characters \n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    #remove special characters, numbers, punctuations\n",
    "   # text = re.sub(r'[^a-zA-Z#\\']', ' ', text)\n",
    "    #removing shortwords\n",
    "    text = ' '.join([w for w in text.split() if len(w)>3])\n",
    "    #lower case \n",
    "    text = text.lower()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting words\n",
    "def extract(x):\n",
    "    words = []\n",
    "    #Loop over the words in the content \n",
    "    for i in x: \n",
    "        word = re.findall(r\"\\b\\w{4,225}\\b\",i)\n",
    "        words.append(word)\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relevant words extraction \n",
    "def relevant(input_word):\n",
    "    word_freq = nltk.FreqDist(input_word)\n",
    "    dict_filter =  lambda word_freq, stopwords: dict((w, word_freq[w]) for w in word_freq if w not in STOPWORDS)\n",
    "    relevant_w = dict_filter(word_freq, STOPWORDS)\n",
    "    \n",
    "    return relevant_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_extraction(filename, appname):\n",
    "    \n",
    "    \n",
    "    \n",
    "    features = pd.read_csv('sentiments_comparison.csv')\n",
    "    dataset = pd.read_csv(str(filename))\n",
    "    #dataset = dataset.drop(columns=['isEdited', 'title', 'date'])\n",
    "    #dataset = dataset.drop(columns=['isEdited', 'title', 'date', 'developerResponse'])\n",
    "    dataset = dataset.drop(columns=['Company_Name', 'URL', 'Category_URL', 'Time'])\n",
    "    \n",
    "    dataset.dropna()\n",
    "    \n",
    "    #dataset['clean_review'] = dataset.review.apply(lambda x: np.vectorize(clean_text)(x))\n",
    "    #dataset['clean_review'] = dataset.content.apply(lambda x: np.vectorize(clean_text)(x))\n",
    "    dataset['clean_review'] = dataset.Comments.apply(lambda x: np.vectorize(clean_text)(x))\n",
    "    \n",
    "    #positive_w = extract(dataset['clean_review'][dataset['rating']>3])\n",
    "    #negative_w = extract(dataset['clean_review'][dataset['rating']<3])\n",
    "    \n",
    "    #positive_w = extract(dataset['clean_review'][dataset['score']>3])\n",
    "    #negative_w = extract(dataset['clean_review'][dataset['score']<3])\n",
    "    \n",
    "    positive_w = extract(dataset['clean_review'][dataset['Star_rating']>3])\n",
    "    negative_w = extract(dataset['clean_review'][dataset['Star_rating']<3])\n",
    "    \n",
    "    positive_w = sum(positive_w, [])\n",
    "    negative_w = sum(negative_w, [])\n",
    "    \n",
    "    allWords = []\n",
    "    allWords.append(positive_w)\n",
    "    allWords.append(negative_w)\n",
    "    allWords = sum(allWords, [])\n",
    "    \n",
    "    relevant_w = relevant(allWords)\n",
    "    \n",
    "    relevant_words = pd.DataFrame({'Word_freq': list(relevant_w.keys()),\n",
    "                              'Word_Count':list(relevant_w.values()),\n",
    "                             })\n",
    "    \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    texts = []\n",
    "    #neg_word_list = []\n",
    "    #neu_word_list = [] \n",
    "    for word1 in relevant_words['Word_freq']:\n",
    "        if(sid.polarity_scores(word1)['compound'])>0.5 or (sid.polarity_scores(word1)['compound'])<-0.5:\n",
    "            texts.append(word1)\n",
    "            \n",
    "    \n",
    "    liking = []\n",
    "    trust = []\n",
    "    anger = []\n",
    "    sadness = []\n",
    "    fear = []\n",
    "    \n",
    "    \n",
    "    l = [] #liking synonyms\n",
    "    t = [] #trust synonyms\n",
    "    a = [] #anger synonyms\n",
    "    sa = [] #sad synonyms\n",
    "    f = [] #fear synonyms\n",
    "\n",
    "    for w in features.liking.values:\n",
    "        for syn in wordnet.synsets(str(w)):\n",
    "            for lemma in syn.lemmas():\n",
    "                l.append(lemma.name())\n",
    "    for txt in texts: \n",
    "        if txt in set(l):\n",
    "            liking.append(txt)\n",
    "    liking = list(dict.fromkeys(liking))\n",
    "    #################################################\n",
    "\n",
    "    for w1 in features.trust.values:\n",
    "        for syn_t in wordnet.synsets(str(w1)):\n",
    "            for lemma1 in syn_t.lemmas():\n",
    "                t.append(lemma1.name())\n",
    "    for txt in texts:\n",
    "        if txt in set(t):\n",
    "            trust.append(txt)\n",
    "    trust = list(dict.fromkeys(trust))\n",
    "    ################################################\n",
    "\n",
    "\n",
    "    for w4 in features.anger.values:\n",
    "        for syn_a in wordnet.synsets(str(w4)):\n",
    "            for lemma4 in syn_a.lemmas():\n",
    "                a.append(lemma4.name())\n",
    "\n",
    "    for txt in texts: \n",
    "        if txt in set(a):\n",
    "            anger.append(txt)\n",
    "    anger = list(dict.fromkeys(anger))\n",
    "    ######################################################\n",
    "\n",
    "    for w5 in features.sadness.values:\n",
    "        for syn_sa in wordnet.synsets(str(w5)):\n",
    "            for lemma5 in syn_sa.lemmas():\n",
    "                sa.append(lemma5.name())\n",
    "    for txt in texts:\n",
    "        if txt in set(sa):\n",
    "            sadness.append(txt)\n",
    "    sadness = list(dict.fromkeys(sadness))\n",
    "\n",
    "    ########################################################\n",
    "    for w6 in features.fear.values:\n",
    "        for syn_f in wordnet.synsets(str(w6)):\n",
    "            for lemma6 in syn_f.lemmas():\n",
    "                f.append(lemma6.name())\n",
    "\n",
    "    for txt in texts:\n",
    "        if txt in set(f):\n",
    "            fear.append(txt)\n",
    "    fear = list(dict.fromkeys(fear))\n",
    "    \n",
    "    for duplicate in liking:\n",
    "        if duplicate in trust:\n",
    "            trust.remove(duplicate)\n",
    "        \n",
    "    for duplicate in trust:\n",
    "        if duplicate in anger:\n",
    "            anger.remove(duplicate)\n",
    "\n",
    "    for duplicate in anger:\n",
    "        if duplicate in sadness:\n",
    "            sadness.remove(duplicate)\n",
    "\n",
    "\n",
    "    for duplicate in sadness:\n",
    "        if duplicate in fear:\n",
    "            fear.remove(duplicate)\n",
    "            \n",
    "    liking_sentiment = pd.DataFrame({'liking':liking})\n",
    "    trust_sentiment = pd.DataFrame({'trust':trust})\n",
    "    anger_sentiment = pd.DataFrame({'anger':anger})\n",
    "    sadness_sentiment = pd.DataFrame({'sadness':sadness})\n",
    "    fear_sentiment = pd.DataFrame({'fear':fear})\n",
    "\n",
    "\n",
    "    sentiments = pd.concat([liking_sentiment, trust_sentiment, anger_sentiment, sadness_sentiment, fear_sentiment], axis = 1)\n",
    "    \n",
    "     #save as csv file in specific folder\n",
    "    sentiments.to_csv(appname, index=None)\n",
    "    return sentiments\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>liking</th>\n",
       "      <th>trust</th>\n",
       "      <th>anger</th>\n",
       "      <th>sadness</th>\n",
       "      <th>fear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>best</td>\n",
       "      <td>NaN</td>\n",
       "      <td>idiot</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>great</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>excellent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>perfect</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>love</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>awesome</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>free</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>superb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>wonderful</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>fantastic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>loved</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>amazing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       liking  trust  anger  sadness  fear\n",
       "0        best    NaN  idiot      NaN   NaN\n",
       "1       great    NaN    NaN      NaN   NaN\n",
       "2   excellent    NaN    NaN      NaN   NaN\n",
       "3     perfect    NaN    NaN      NaN   NaN\n",
       "4        love    NaN    NaN      NaN   NaN\n",
       "5     awesome    NaN    NaN      NaN   NaN\n",
       "6        free    NaN    NaN      NaN   NaN\n",
       "7      superb    NaN    NaN      NaN   NaN\n",
       "8   beautiful    NaN    NaN      NaN   NaN\n",
       "9   wonderful    NaN    NaN      NaN   NaN\n",
       "10  fantastic    NaN    NaN      NaN   NaN\n",
       "11      loved    NaN    NaN      NaN   NaN\n",
       "12    amazing    NaN    NaN      NaN   NaN"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_extraction('../../mHealth_google_app/A39 - e-Anatomy.csv', 'app_sentiments/A39_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
